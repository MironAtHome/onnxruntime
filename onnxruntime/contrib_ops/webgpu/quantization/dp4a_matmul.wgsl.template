// Copyright (c) Microsoft Corporation. All rights reserved.
// Licensed under the MIT License.

#param block_size
#param n_bits
#param has_zero_points

#include "quantization/dp4a_matmul_common.wgsl.template"

// This shader implements co-operative matrix multiply. The key idea here is to
// assume there is a primitive for medium size matrix multiply a subgroup can perform,
// using all its lanes and pooling all its registers to keep the values in registry.
//
// The entire workgroup which has N subgroups first loads a tile into shared memory,
// Then each subgroup loads a subtile from shared memory into registers and uses
// the medium size matrix multiply primitive to perform the math.
// The values for tile/subtile size are chosen to conform to the resource limits
// of an alderlake/tiger lake gpu. A tile is 64x64, workgroup is 256 threads -
// therefore there are 16 subgroups and 16 lanes in each subgroup.
// K the hidden dimension is paged in from RAM at k tile size which is 64.
// All this puts the shared memory requirement slightly above 16KB.
// WebGPU limit is 16KB, output is moved to registers instead of SHM to make
// everything fit in shared memory.
//
// Each subgroup performs a 16 x 64 x 16 multiply which is implemented with
// subgroup shuffle as a placeholder for the day the medium matrix mul primitive
// becomes available in WGSL. The registry requirements is ~2KB per subgroup, on
// Alderlake/Tigerlake subgroup has 8KB of registry space pooling the
// 512B of registry from each lane.
//
// The medium size matmul is implemented using dot4I8Packed, so the inputs for
// this shader require A to be int8 quantized with block size 64. B is regular
// matmulnbits input with block size 32.

const tile_size = 64;
const subtile_size = 16;
const tile_size_k =  32;
const vec_factor = 4;
const u32_factor = 4;
const tile_size_k_vec = 2;

// Shared memory
var<workgroup> tile_A : array<array<vec4<u32>, tile_size>, tile_size_k_vec>;                     // 64 x 32
var<workgroup> scale_A : array<output_element_t, tile_size>;                                     // 64 x 1
var<workgroup> tile_B : array<array<vec4<u32>, tile_size>, tile_size_k_vec>;                     // 64 x 32
var<workgroup> scale_B : array<output_element_t, tile_size>;                                     // 64 x 1

#if has_zero_points
    var<workgroup> zeroes : array<i32, tile_size>;
#endif

fn loadSHMA(a_global_base:u32, kidx_v:u32, row: u32, col: u32)
{
    let a_global = a_global_base + row;
    if (a_global >= uniforms.M)
    {
        return;
    }
    tile_A[col][row] = input_a[a_global*uniforms.K16+kidx_v+col];
    if (col == 0)
    {
        // kidx_v - covers 16 values of k
        scale_A[row] = scales_a[a_global*(uniforms.K/128) + kidx_v/8];
    }
}

#if n_bits == 4
    fn loadSHMB(b_global_base:u32, kidx_v:u32, row: u32, col: u32)
    {
        let b_global = b_global_base + row;
        if (b_global >= uniforms.N)
        {
            return;
        }

        let b_value = input_b[b_global*uniforms.K16+kidx_v+col];
        let block_idx = kidx_v/(block_size/16);
        let zero = mm_read_zero(b_global, block_idx, uniforms.N, uniforms.zero_blocks_per_col);
        tile_B[col][row] = DequantizedFrom4BitsTo8Bits(b_value, zero);
        if (col == 0)
        {
            // kidx_v - each kidx_v covers 16 values of k
            scale_B[row] = scales_b[b_global*(uniforms.K/block_size) + block_idx];
        }
    }
#endif

#if n_bits == 8
    fn loadSHMB(b_global_base:u32, kidx_v:u32, row: u32, col: u32)
    {
        let b_global = b_global_base + row;
        if (b_global >= uniforms.N)
        {
            return;
        }

        let b_value = input_b[b_global*uniforms.K16+kidx_v+col];
        tile_B[col][row] = AlignWithZeroPoint(b_value);
        if (col == 0)
        {
            // kidx_v - each kidx_v covers 16 values of k
            let block_idx = kidx_v/(block_size/16);
            scale_B[row] = scales_b[b_global*(uniforms.K/block_size) + block_idx];
#if has_zero_points
            zeroes[row] = mm_read_zero(b_global, block_idx, uniforms.N, uniforms.zero_blocks_per_col);
#endif
        }
    }
#endif

#if n_bits == 2
    const lut_size = 256;
    var<workgroup> shm_dequantization_table  : array<u32, lut_size>;
    const q2_dequantization_table = array<u32, lut_size>(
        0xFEFEFEFE,
        0xFEFEFEFF,
        0xFEFEFE00,
        0xFEFEFE01,
        0xFEFEFFFE,
        0xFEFEFFFF,
        0xFEFEFF00,
        0xFEFEFF01,
        0xFEFE00FE,
        0xFEFE00FF,
        0xFEFE0000,
        0xFEFE0001,
        0xFEFE01FE,
        0xFEFE01FF,
        0xFEFE0100,
        0xFEFE0101,
        0xFEFFFEFE,
        0xFEFFFEFF,
        0xFEFFFE00,
        0xFEFFFE01,
        0xFEFFFFFE,
        0xFEFFFFFF,
        0xFEFFFF00,
        0xFEFFFF01,
        0xFEFF00FE,
        0xFEFF00FF,
        0xFEFF0000,
        0xFEFF0001,
        0xFEFF01FE,
        0xFEFF01FF,
        0xFEFF0100,
        0xFEFF0101,
        0xFE00FEFE,
        0xFE00FEFF,
        0xFE00FE00,
        0xFE00FE01,
        0xFE00FFFE,
        0xFE00FFFF,
        0xFE00FF00,
        0xFE00FF01,
        0xFE0000FE,
        0xFE0000FF,
        0xFE000000,
        0xFE000001,
        0xFE0001FE,
        0xFE0001FF,
        0xFE000100,
        0xFE000101,
        0xFE01FEFE,
        0xFE01FEFF,
        0xFE01FE00,
        0xFE01FE01,
        0xFE01FFFE,
        0xFE01FFFF,
        0xFE01FF00,
        0xFE01FF01,
        0xFE0100FE,
        0xFE0100FF,
        0xFE010000,
        0xFE010001,
        0xFE0101FE,
        0xFE0101FF,
        0xFE010100,
        0xFE010101,
        0xFFFEFEFE,
        0xFFFEFEFF,
        0xFFFEFE00,
        0xFFFEFE01,
        0xFFFEFFFE,
        0xFFFEFFFF,
        0xFFFEFF00,
        0xFFFEFF01,
        0xFFFE00FE,
        0xFFFE00FF,
        0xFFFE0000,
        0xFFFE0001,
        0xFFFE01FE,
        0xFFFE01FF,
        0xFFFE0100,
        0xFFFE0101,
        0xFFFFFEFE,
        0xFFFFFEFF,
        0xFFFFFE00,
        0xFFFFFE01,
        0xFFFFFFFE,
        0xFFFFFFFF,
        0xFFFFFF00,
        0xFFFFFF01,
        0xFFFF00FE,
        0xFFFF00FF,
        0xFFFF0000,
        0xFFFF0001,
        0xFFFF01FE,
        0xFFFF01FF,
        0xFFFF0100,
        0xFFFF0101,
        0xFF00FEFE,
        0xFF00FEFF,
        0xFF00FE00,
        0xFF00FE01,
        0xFF00FFFE,
        0xFF00FFFF,
        0xFF00FF00,
        0xFF00FF01,
        0xFF0000FE,
        0xFF0000FF,
        0xFF000000,
        0xFF000001,
        0xFF0001FE,
        0xFF0001FF,
        0xFF000100,
        0xFF000101,
        0xFF01FEFE,
        0xFF01FEFF,
        0xFF01FE00,
        0xFF01FE01,
        0xFF01FFFE,
        0xFF01FFFF,
        0xFF01FF00,
        0xFF01FF01,
        0xFF0100FE,
        0xFF0100FF,
        0xFF010000,
        0xFF010001,
        0xFF0101FE,
        0xFF0101FF,
        0xFF010100,
        0xFF010101,
        0x00FEFEFE,
        0x00FEFEFF,
        0x00FEFE00,
        0x00FEFE01,
        0x00FEFFFE,
        0x00FEFFFF,
        0x00FEFF00,
        0x00FEFF01,
        0x00FE00FE,
        0x00FE00FF,
        0x00FE0000,
        0x00FE0001,
        0x00FE01FE,
        0x00FE01FF,
        0x00FE0100,
        0x00FE0101,
        0x00FFFEFE,
        0x00FFFEFF,
        0x00FFFE00,
        0x00FFFE01,
        0x00FFFFFE,
        0x00FFFFFF,
        0x00FFFF00,
        0x00FFFF01,
        0x00FF00FE,
        0x00FF00FF,
        0x00FF0000,
        0x00FF0001,
        0x00FF01FE,
        0x00FF01FF,
        0x00FF0100,
        0x00FF0101,
        0x0000FEFE,
        0x0000FEFF,
        0x0000FE00,
        0x0000FE01,
        0x0000FFFE,
        0x0000FFFF,
        0x0000FF00,
        0x0000FF01,
        0x000000FE,
        0x000000FF,
        0x00000000,
        0x00000001,
        0x000001FE,
        0x000001FF,
        0x00000100,
        0x00000101,
        0x0001FEFE,
        0x0001FEFF,
        0x0001FE00,
        0x0001FE01,
        0x0001FFFE,
        0x0001FFFF,
        0x0001FF00,
        0x0001FF01,
        0x000100FE,
        0x000100FF,
        0x00010000,
        0x00010001,
        0x000101FE,
        0x000101FF,
        0x00010100,
        0x00010101,
        0x01FEFEFE,
        0x01FEFEFF,
        0x01FEFE00,
        0x01FEFE01,
        0x01FEFFFE,
        0x01FEFFFF,
        0x01FEFF00,
        0x01FEFF01,
        0x01FE00FE,
        0x01FE00FF,
        0x01FE0000,
        0x01FE0001,
        0x01FE01FE,
        0x01FE01FF,
        0x01FE0100,
        0x01FE0101,
        0x01FFFEFE,
        0x01FFFEFF,
        0x01FFFE00,
        0x01FFFE01,
        0x01FFFFFE,
        0x01FFFFFF,
        0x01FFFF00,
        0x01FFFF01,
        0x01FF00FE,
        0x01FF00FF,
        0x01FF0000,
        0x01FF0001,
        0x01FF01FE,
        0x01FF01FF,
        0x01FF0100,
        0x01FF0101,
        0x0100FEFE,
        0x0100FEFF,
        0x0100FE00,
        0x0100FE01,
        0x0100FFFE,
        0x0100FFFF,
        0x0100FF00,
        0x0100FF01,
        0x010000FE,
        0x010000FF,
        0x01000000,
        0x01000001,
        0x010001FE,
        0x010001FF,
        0x01000100,
        0x01000101,
        0x0101FEFE,
        0x0101FEFF,
        0x0101FE00,
        0x0101FE01,
        0x0101FFFE,
        0x0101FFFF,
        0x0101FF00,
        0x0101FF01,
        0x010100FE,
        0x010100FF,
        0x01010000,
        0x01010001,
        0x010101FE,
        0x010101FF,
        0x01010100,
        0x01010101);
    fn loadSHMB(b_global_base:u32, kidx_v:u32, row: u32, col: u32)
    {
        let b_global = b_global_base + row;
        if (b_global >= uniforms.N)
        {
            return;
        }
        let b_value = input_b[b_global*uniforms.K16+kidx_v+col];
        tile_B[col][row][0] = shm_dequantization_table[b_value & 0x000000FFu];
        tile_B[col][row][1] = shm_dequantization_table[b_value & 0x0000FF00u];
        tile_B[col][row][2] = shm_dequantization_table[b_value & 0x00FF0000u];
        tile_B[col][row][3] = shm_dequantization_table[b_value & 0xFF000000u];
        let block_idx = kidx_v/(block_size/16);
        scale_B[row] = scales_b[b_global*(uniforms.K/block_size) + block_idx];
    }
#endif

$MAIN {
#if n_bits == 2
    // Move dequantization table into on chip memory.
    shm_dequantization_table[local_idx] = q2_dequantization_table[local_idx];
    workgroupBarrier();
#endif
    // During the load phase we use all 256 threads to load 64 rows of A/B.
    // For each row we load tile_size_k_vec (2) vectorized elements, which are 32 elements of K.
    let a_global_base = u32(workgroup_idx / uniforms.num_N_tile) * tile_size;
    let b_global_base = (workgroup_idx % uniforms.num_N_tile) * tile_size;
    let load_AorB = u32(local_idx/128);
    let load_row = u32((local_idx%128)/2);
    let load_col = u32(local_idx%2);

    // During the compute phase, we have the 64x64 tile split into
    // subtiles of 16x16. We have a grid of 4x4 subtiles.
    let subtile_id = u32(local_idx / subtile_size);
    let subtile_idx = u32(subtile_id / 4);
    let subtile_idy = u32(subtile_id % 4);
    let base_A = subtile_idx * 16;
    let base_B = subtile_idy * 16;
    // For each subtile we have 16 threads assigned.
    let a_idx = u32(local_idx % subtile_size);

    var lane_output1: vec4<output_element_t>;
    var lane_output2: vec4<output_element_t>;
    var lane_output3: vec4<output_element_t>;
    var lane_output4: vec4<output_element_t>;
    // K's vectorization is 16 items per index. See input_a/input_b.
    // tile_size_k_vec - is the k tile size in vectorized space (1/16). That is
    // k tile size is 32. In vectorized space that is 32/16 = 2.
    for (var kidx_v:u32 = 0; kidx_v < uniforms.K16; kidx_v+=tile_size_k_vec)
    {
        // Load Phase: Populate shared memory for the workgroup.
        if (load_AorB == 0)
        {
            loadSHMA(a_global_base, kidx_v, load_row, load_col);
        }
        else
        {
            loadSHMB(b_global_base, kidx_v, load_row, load_col);
        }
        workgroupBarrier();

        // Compute phase: Perform matmul for this subtile 16 x 32 x 16.
        // Step 1: Load from shared memory into registers across entire subgroup.
        var own_a0: vec4<u32> = tile_A[0][base_A + a_idx];
        var own_a1: vec4<u32> = tile_A[1][base_A + a_idx];
        var own_scale_a: output_element_t = scale_A[base_A + a_idx];

#if has_zero_points && n_bits == 8
        if (sg_size == 16)
        {
            var own_b0: vec4<u32> = tile_B[0][base_B + sg_id];
            var own_b1: vec4<u32> = tile_B[1][base_B + sg_id];
            var own_scale_b: output_element_t  = scale_B[base_B + sg_id];
            var zero = zeroes[base_B + sg_id];
            // Step 2: Access registers across the subgroup using subgroupShuffle and perform the matmul.
            lane_output1[0] += SDP8AI(own_a0, subgroupShuffle(own_b0, 0), own_a1, subgroupShuffle(own_b1, 0), subgroupShuffle(own_scale_b, 0) * own_scale_a, subgroupShuffle(zero, 0));
            lane_output1[1] += SDP8AI(own_a0, subgroupShuffle(own_b0, 1), own_a1, subgroupShuffle(own_b1, 1), subgroupShuffle(own_scale_b, 1) * own_scale_a, subgroupShuffle(zero, 1));
            lane_output1[2] += SDP8AI(own_a0, subgroupShuffle(own_b0, 2), own_a1, subgroupShuffle(own_b1, 2), subgroupShuffle(own_scale_b, 2) * own_scale_a, subgroupShuffle(zero, 2));
            lane_output1[3] += SDP8AI(own_a0, subgroupShuffle(own_b0, 3), own_a1, subgroupShuffle(own_b1, 3), subgroupShuffle(own_scale_b, 3) * own_scale_a, subgroupShuffle(zero, 3));

            lane_output2[0] += SDP8AI(own_a0, subgroupShuffle(own_b0, 4), own_a1, subgroupShuffle(own_b1, 4), subgroupShuffle(own_scale_b, 4) * own_scale_a, subgroupShuffle(zero, 4));
            lane_output2[1] += SDP8AI(own_a0, subgroupShuffle(own_b0, 5), own_a1, subgroupShuffle(own_b1, 5), subgroupShuffle(own_scale_b, 5) * own_scale_a, subgroupShuffle(zero, 5));
            lane_output2[2] += SDP8AI(own_a0, subgroupShuffle(own_b0, 6), own_a1, subgroupShuffle(own_b1, 6), subgroupShuffle(own_scale_b, 6) * own_scale_a, subgroupShuffle(zero, 6));
            lane_output2[3] += SDP8AI(own_a0, subgroupShuffle(own_b0, 7), own_a1, subgroupShuffle(own_b1, 7), subgroupShuffle(own_scale_b, 7) * own_scale_a, subgroupShuffle(zero, 7));

            lane_output3[0] += SDP8AI(own_a0, subgroupShuffle(own_b0, 8), own_a1, subgroupShuffle(own_b1, 8), subgroupShuffle(own_scale_b, 8) * own_scale_a, subgroupShuffle(zero, 8));
            lane_output3[1] += SDP8AI(own_a0, subgroupShuffle(own_b0, 9), own_a1, subgroupShuffle(own_b1, 9), subgroupShuffle(own_scale_b, 9) * own_scale_a, subgroupShuffle(zero, 9));
            lane_output3[2] += SDP8AI(own_a0, subgroupShuffle(own_b0, 10), own_a1, subgroupShuffle(own_b1, 10), subgroupShuffle(own_scale_b, 10) * own_scale_a, subgroupShuffle(zero, 10));
            lane_output3[3] += SDP8AI(own_a0, subgroupShuffle(own_b0, 11), own_a1, subgroupShuffle(own_b1, 11), subgroupShuffle(own_scale_b, 11) * own_scale_a, subgroupShuffle(zero, 11));

            lane_output4[0] += SDP8AI(own_a0, subgroupShuffle(own_b0, 12), own_a1, subgroupShuffle(own_b1, 12), subgroupShuffle(own_scale_b, 12) * own_scale_a, subgroupShuffle(zero, 12));
            lane_output4[1] += SDP8AI(own_a0, subgroupShuffle(own_b0, 13), own_a1, subgroupShuffle(own_b1, 13), subgroupShuffle(own_scale_b, 13) * own_scale_a, subgroupShuffle(zero, 13));
            lane_output4[2] += SDP8AI(own_a0, subgroupShuffle(own_b0, 14), own_a1, subgroupShuffle(own_b1, 14), subgroupShuffle(own_scale_b, 14) * own_scale_a, subgroupShuffle(zero, 14));
            lane_output4[3] += SDP8AI(own_a0, subgroupShuffle(own_b0, 15), own_a1, subgroupShuffle(own_b1, 15), subgroupShuffle(own_scale_b, 15) * own_scale_a, subgroupShuffle(zero, 15));
        }
        else
        {
            // Code for other subgroup sizes, simply doesnt use subgroups at all.
            // Relies on reads from single location tile_B[][base_B + col] by all
            // being optimized by the hardware.
            lane_output1[0] += SDP8AI(own_a0, tile_B[0][base_B + 0], own_a1, tile_B[1][base_B + 0],  own_scale_a * scale_B[base_B + 0], zeroes[base_B + 0]);
            lane_output1[1] += SDP8AI(own_a0, tile_B[0][base_B + 1], own_a1, tile_B[1][base_B + 1],  own_scale_a * scale_B[base_B + 1], zeroes[base_B + 1]);
            lane_output1[2] += SDP8AI(own_a0, tile_B[0][base_B + 2], own_a1, tile_B[1][base_B + 2],  own_scale_a * scale_B[base_B + 2], zeroes[base_B + 2]);
            lane_output1[3] += SDP8AI(own_a0, tile_B[0][base_B + 3], own_a1, tile_B[1][base_B + 3],  own_scale_a * scale_B[base_B + 3], zeroes[base_B + 3]);

            lane_output2[0] += SDP8AI(own_a0, tile_B[0][base_B + 4], own_a1, tile_B[1][base_B + 4],  own_scale_a * scale_B[base_B + 4], zeroes[base_B + 4]);
            lane_output2[1] += SDP8AI(own_a0, tile_B[0][base_B + 5], own_a1, tile_B[1][base_B + 5],  own_scale_a * scale_B[base_B + 5], zeroes[base_B + 5]);
            lane_output2[2] += SDP8AI(own_a0, tile_B[0][base_B + 6], own_a1, tile_B[1][base_B + 6],  own_scale_a * scale_B[base_B + 6], zeroes[base_B + 6]);
            lane_output2[3] += SDP8AI(own_a0, tile_B[0][base_B + 7], own_a1, tile_B[1][base_B + 7],  own_scale_a * scale_B[base_B + 7], zeroes[base_B + 7]);

            lane_output3[0] += SDP8AI(own_a0, tile_B[0][base_B + 8], own_a1, tile_B[1][base_B + 8],  own_scale_a * scale_B[base_B + 8], zeroes[base_B + 8]);
            lane_output3[1] += SDP8AI(own_a0, tile_B[0][base_B + 9], own_a1, tile_B[1][base_B + 9],  own_scale_a * scale_B[base_B + 9], zeroes[base_B + 9]);
            lane_output3[2] += SDP8AI(own_a0, tile_B[0][base_B + 10], own_a1, tile_B[1][base_B + 10],  own_scale_a * scale_B[base_B + 10], zeroes[base_B + 10]);
            lane_output3[3] += SDP8AI(own_a0, tile_B[0][base_B + 11], own_a1, tile_B[1][base_B + 11],  own_scale_a * scale_B[base_B + 11], zeroes[base_B + 11]);

            lane_output4[0] += SDP8AI(own_a0, tile_B[0][base_B + 12], own_a1, tile_B[1][base_B + 12],  own_scale_a * scale_B[base_B + 12], zeroes[base_B + 12]);
            lane_output4[1] += SDP8AI(own_a0, tile_B[0][base_B + 13], own_a1, tile_B[1][base_B + 13],  own_scale_a * scale_B[base_B + 13], zeroes[base_B + 13]);
            lane_output4[2] += SDP8AI(own_a0, tile_B[0][base_B + 14], own_a1, tile_B[1][base_B + 14],  own_scale_a * scale_B[base_B + 14], zeroes[base_B + 14]);
            lane_output4[3] += SDP8AI(own_a0, tile_B[0][base_B + 15], own_a1, tile_B[1][base_B + 15],  own_scale_a * scale_B[base_B + 15], zeroes[base_B + 15]);
        }
#else
        if (sg_size == 16)
        {
            var own_b0: vec4<u32> = tile_B[0][base_B + sg_id];
            var own_b1: vec4<u32> = tile_B[1][base_B + sg_id];
            var own_scale_b: output_element_t  = scale_B[base_B + sg_id];
            // Step 2: Access registers across the subgroup using subgroupShuffle and perform the matmul.
            lane_output1[0] += SDP8AI(own_a0, subgroupShuffle(own_b0, 0), own_a1, subgroupShuffle(own_b1, 0), subgroupShuffle(own_scale_b, 0) * own_scale_a);
            lane_output1[1] += SDP8AI(own_a0, subgroupShuffle(own_b0, 1), own_a1, subgroupShuffle(own_b1, 1), subgroupShuffle(own_scale_b, 1) * own_scale_a);
            lane_output1[2] += SDP8AI(own_a0, subgroupShuffle(own_b0, 2), own_a1, subgroupShuffle(own_b1, 2), subgroupShuffle(own_scale_b, 2) * own_scale_a);
            lane_output1[3] += SDP8AI(own_a0, subgroupShuffle(own_b0, 3), own_a1, subgroupShuffle(own_b1, 3), subgroupShuffle(own_scale_b, 3) * own_scale_a);

            lane_output2[0] += SDP8AI(own_a0, subgroupShuffle(own_b0, 4), own_a1, subgroupShuffle(own_b1, 4), subgroupShuffle(own_scale_b, 4) * own_scale_a);
            lane_output2[1] += SDP8AI(own_a0, subgroupShuffle(own_b0, 5), own_a1, subgroupShuffle(own_b1, 5), subgroupShuffle(own_scale_b, 5) * own_scale_a);
            lane_output2[2] += SDP8AI(own_a0, subgroupShuffle(own_b0, 6), own_a1, subgroupShuffle(own_b1, 6), subgroupShuffle(own_scale_b, 6) * own_scale_a);
            lane_output2[3] += SDP8AI(own_a0, subgroupShuffle(own_b0, 7), own_a1, subgroupShuffle(own_b1, 7), subgroupShuffle(own_scale_b, 7) * own_scale_a);

            lane_output3[0] += SDP8AI(own_a0, subgroupShuffle(own_b0, 8), own_a1, subgroupShuffle(own_b1, 8), subgroupShuffle(own_scale_b, 8) * own_scale_a);
            lane_output3[1] += SDP8AI(own_a0, subgroupShuffle(own_b0, 9), own_a1, subgroupShuffle(own_b1, 9), subgroupShuffle(own_scale_b, 9) * own_scale_a);
            lane_output3[2] += SDP8AI(own_a0, subgroupShuffle(own_b0, 10), own_a1, subgroupShuffle(own_b1, 10), subgroupShuffle(own_scale_b, 10) * own_scale_a);
            lane_output3[3] += SDP8AI(own_a0, subgroupShuffle(own_b0, 11), own_a1, subgroupShuffle(own_b1, 11), subgroupShuffle(own_scale_b, 11) * own_scale_a);

            lane_output4[0] += SDP8AI(own_a0, subgroupShuffle(own_b0, 12), own_a1, subgroupShuffle(own_b1, 12), subgroupShuffle(own_scale_b, 12) * own_scale_a);
            lane_output4[1] += SDP8AI(own_a0, subgroupShuffle(own_b0, 13), own_a1, subgroupShuffle(own_b1, 13), subgroupShuffle(own_scale_b, 13) * own_scale_a);
            lane_output4[2] += SDP8AI(own_a0, subgroupShuffle(own_b0, 14), own_a1, subgroupShuffle(own_b1, 14), subgroupShuffle(own_scale_b, 14) * own_scale_a);
            lane_output4[3] += SDP8AI(own_a0, subgroupShuffle(own_b0, 15), own_a1, subgroupShuffle(own_b1, 15), subgroupShuffle(own_scale_b, 15) * own_scale_a);
        }
        else
        {
            // Code for other subgroup sizes, simply doesnt use subgroups at all.
            // Relies on reads from single location tile_B[][base_B + col] by all
            // being optimized by the hardware.
            lane_output1[0] += SDP8AI(own_a0, tile_B[0][base_B + 0], own_a1, tile_B[1][base_B + 0],  own_scale_a * scale_B[base_B + 0]);
            lane_output1[1] += SDP8AI(own_a0, tile_B[0][base_B + 1], own_a1, tile_B[1][base_B + 1],  own_scale_a * scale_B[base_B + 1]);
            lane_output1[2] += SDP8AI(own_a0, tile_B[0][base_B + 2], own_a1, tile_B[1][base_B + 2],  own_scale_a * scale_B[base_B + 2]);
            lane_output1[3] += SDP8AI(own_a0, tile_B[0][base_B + 3], own_a1, tile_B[1][base_B + 3],  own_scale_a * scale_B[base_B + 3]);

            lane_output2[0] += SDP8AI(own_a0, tile_B[0][base_B + 4], own_a1, tile_B[1][base_B + 4],  own_scale_a * scale_B[base_B + 4]);
            lane_output2[1] += SDP8AI(own_a0, tile_B[0][base_B + 5], own_a1, tile_B[1][base_B + 5],  own_scale_a * scale_B[base_B + 5]);
            lane_output2[2] += SDP8AI(own_a0, tile_B[0][base_B + 6], own_a1, tile_B[1][base_B + 6],  own_scale_a * scale_B[base_B + 6]);
            lane_output2[3] += SDP8AI(own_a0, tile_B[0][base_B + 7], own_a1, tile_B[1][base_B + 7],  own_scale_a * scale_B[base_B + 7]);

            lane_output3[0] += SDP8AI(own_a0, tile_B[0][base_B + 8], own_a1, tile_B[1][base_B + 8],  own_scale_a * scale_B[base_B + 8]);
            lane_output3[1] += SDP8AI(own_a0, tile_B[0][base_B + 9], own_a1, tile_B[1][base_B + 9],  own_scale_a * scale_B[base_B + 9]);
            lane_output3[2] += SDP8AI(own_a0, tile_B[0][base_B + 10], own_a1, tile_B[1][base_B + 10],  own_scale_a * scale_B[base_B + 10]);
            lane_output3[3] += SDP8AI(own_a0, tile_B[0][base_B + 11], own_a1, tile_B[1][base_B + 11],  own_scale_a * scale_B[base_B + 11]);

            lane_output4[0] += SDP8AI(own_a0, tile_B[0][base_B + 12], own_a1, tile_B[1][base_B + 12],  own_scale_a * scale_B[base_B + 12]);
            lane_output4[1] += SDP8AI(own_a0, tile_B[0][base_B + 13], own_a1, tile_B[1][base_B + 13],  own_scale_a * scale_B[base_B + 13]);
            lane_output4[2] += SDP8AI(own_a0, tile_B[0][base_B + 14], own_a1, tile_B[1][base_B + 14],  own_scale_a * scale_B[base_B + 14]);
            lane_output4[3] += SDP8AI(own_a0, tile_B[0][base_B + 15], own_a1, tile_B[1][base_B + 15],  own_scale_a * scale_B[base_B + 15]);
        }
#endif
        workgroupBarrier();
    }

    let a_global = a_global_base + base_A + a_idx;
    let b_global = b_global_base + base_B;
    let output_idx = ((a_global) * uniforms.N + b_global)/4;
    // This creates a shader requirement that uniforms.N % 16 == 0
    if (a_global < uniforms.M && b_global < uniforms.N)
    {
        output[output_idx] = lane_output1;
        output[output_idx+1] = lane_output2;
        output[output_idx+2] = lane_output3;
        output[output_idx+3] = lane_output4;
    }
}  // MAIN
